# çŸ¥è¯†ç‚¹
- RDDçš„ç›¸å…³æ¦‚å¿µï¼Œäº†è§£HDFSçš„åˆ†å¸ƒå¼å­˜å‚¨ï¼Œmapreduceçš„æ€æƒ³ï¼Œç¨‹åºçš„å¹¶è¡ŒåŒ–è¿›è¡Œ
- Sparkåˆ†ä¸ºè½¬æ¢æ“ä½œå’ŒåŠ¨ä½œï¼ŒDAGçš„èµ„æºè®¡ç®—å›¾åªæœ‰åœ¨actionçš„æ—¶å€™æ‰ä¼šè§¦å‘è¿ç®—ï¼Œåªæœ‰ä¸­é—´ç»“æžœè¢«cacheã€persisçš„æ—¶å€™æ‰ä¼šæš‚å­˜ï¼Œå¦åˆ™ä¼šé‡å¤è®¡ç®—
- broadcastå’Œaccumulatorå…±äº«å˜é‡ï¼Œå±€éƒ¨çš„globalä¹Ÿæ˜¯localçš„
- sparkçš„DFæ“ä½œ
- sparkçš„graphæ“ä½œ
- sparkçš„æœºå™¨å­¦ä¹ åº“
- streamæ“ä½œ
- Hbaseå’ŒHive

### å¹¶è¡Œç¨‹åºé—®é¢˜ï¼š 
- æ­»é”é—®é¢˜ï¼šä¿¡å·é‡+äº’æ–¥é”  
- restricted - çº¦æŸ   deterministic - ç¡®å®šçš„
### RDD  
Spark æ ¸å¿ƒçš„æ¦‚å¿µæ˜¯ Resilient Distributed Dataset (RDD)ï¼šä¸€ä¸ªå¯å¹¶è¡Œæ“ä½œçš„æœ‰å®¹é”™æœºåˆ¶çš„æ•°æ®é›†åˆã€‚æœ‰ 2 ç§æ–¹å¼åˆ›å»º RDDsï¼šç¬¬ä¸€ç§æ˜¯åœ¨ä½ çš„é©±åŠ¨ç¨‹åºä¸­å¹¶è¡ŒåŒ–ä¸€ä¸ªå·²ç»å­˜åœ¨çš„é›†åˆï¼›å¦å¤–ä¸€ç§æ˜¯å¼•ç”¨ä¸€ä¸ªå¤–éƒ¨å­˜å‚¨ç³»ç»Ÿçš„æ•°æ®é›†ï¼Œä¾‹å¦‚å…±äº«çš„æ–‡ä»¶ç³»ç»Ÿï¼ŒHDFSï¼ŒHBaseæˆ–å…¶ä»– Hadoop æ•°æ®æ ¼å¼çš„æ•°æ®æºã€‚ï¼ˆtextfileï¼‰    
partitionæ˜¯spark rddè®¡ç®—çš„æœ€å°å•å…ƒã€‚  
Spark RDDä¸»è¦ç”±Dependencyã€Partitionã€Partitionerç»„æˆã€‚  
    1. Partitionè®°å½•äº†æ•°æ®splitçš„é€»è¾‘ï¼Œ  
    2. Dependencyè®°å½•çš„æ˜¯transformationæ“ä½œè¿‡ç¨‹ä¸­Partitionçš„æ¼”åŒ–ï¼Œ  
    3. Partitioneræ˜¯shuffleè¿‡ç¨‹ä¸­keyé‡åˆ†åŒºæ—¶çš„ç­–ç•¥ï¼Œå³è®¡ç®—keyå†³å®šk-vå±žäºŽå“ªä¸ªåˆ†åŒº  
#### å¹¶è¡ŒåŒ–  
å¹¶è¡Œé›†åˆ (Parallelized collections) çš„åˆ›å»ºæ˜¯é€šè¿‡åœ¨ä¸€ä¸ªå·²æœ‰çš„é›†åˆ(Scala Seq)ä¸Šè°ƒç”¨ SparkContext çš„ parallelize æ–¹æ³•å®žçŽ°çš„ã€‚é›†åˆä¸­çš„å…ƒç´ è¢«å¤åˆ¶åˆ°ä¸€ä¸ªå¯å¹¶è¡Œæ“ä½œçš„åˆ†å¸ƒå¼æ•°æ®é›†ä¸­ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ¼”ç¤ºäº†å¦‚ä½•åœ¨ä¸€ä¸ªåŒ…å« 1 åˆ° 5 çš„æ•°ç»„ä¸­åˆ›å»ºå¹¶è¡Œé›†åˆï¼š
```python
val data = Array(1, 2, 3, 4, 5)
val distData = sc.parallelize(data)
```
### Map-Reduce  
- store: GFS-google distribute file system  
### Hadoop  
- open source of google GFS and MR  
- Master(task tracker+name node | 64MB chunk å­—å…¸) + Slave(task tracker+data node å­˜å‚¨chunk,å¤åˆ¶ä¸‰ä»½ï¼Œåœ¨ä¸åŒçš„slaver nodeä¸­)  
- HDFS+Hadoop MR  
- å¿ƒè·³ 
- Map-> åˆ†é…åˆ°ä¸åŒçš„worker|Prase-hash : convert str into a number(means the index of woker) -> Reduce
- ç¼ºç‚¹ï¼š 
> lost transaction features etc.  
> æš´åŠ›ä»£æ›¿ç´¢å¼•  
- ç»„æˆ  
> master: **job tracker**    NameNode  
> worker(slaver):map-reduce+ DataNode  **task tracker**  

### Spark
- features:  
rich api;in-memory storage;**execution graph**;interactive shell  
- Structure and features  
çº¿æ€§è®¡ç®—æµå›¾ï¼Œè®¡ç®—å®Œæ¯•ä¼šä¸¢å¼ƒç»“æžœï¼Œå†ä½¿ç”¨ä¼šæ ¹æ®è®¡ç®—å›¾ä»Žå¦‚ä½•äº§ç”Ÿçš„åœ°æ–¹å†è®¡ç®—ä¸€é
- è½¬æ¢æ“ä½œï¼š Transform:,map,filter,repartition,union,distinct
    - mapç­‰ï¼Œæ¯ä¸€ä¸ªæ•°æ®é›†å…ƒç´ ä¼ é€’ç»™ä¸€ä¸ªå‡½æ•°å¹¶ä¸”è¿”å›žä¸€ä¸ªæ–°çš„ RDDï¼Œå¹¶ä¸ä¼šç«‹å³è¿›è¡Œè®¡ç®—ï¼Œå¹¶ä¸”è½¬æ¢çš„ä¸­é—´ç»“æžœé»˜è®¤ä¸€æ¬¡æ€§
- Actionï¼šreduce,count,first,takeSample
    - reduce æ˜¯ä¸€ä¸ªåŠ¨ä½œï¼Œé›†åˆæ‰€æœ‰å…ƒç´ å¹¶è¿”å›ž
- still HDFS layer  
RDD store 1 copy not 3 like hdfs  
master separate RDDs  
data transpart via pipeline,to learner formed different rdds, no need to store intermediate results  
if one failed, just recompute in parallel and in different nodes from compute graph, not rollback.  
lines.cache()/persist(): æš‚å­˜ï¼Œè®¡ç®—å®Œä¸ä¼šç«‹é©¬è¢«ä¸¢å¼ƒï¼Œä»Žè€Œé¿å…è®¡ç®—å¤šé  
LRU ä¸¢å¼ƒæœ€ä¹…æœªä½¿ç”¨çš„æ—§æ•°æ® .å³ä½¿åŽŸå§‹æ•°æ®å˜äº†ï¼Œä¹Ÿæ²¡æœ‰å…³ç³»  
python global varibale ä¼šè¢«é€åˆ°æ¯ä¸€ä¸ªexecutor,pythonå‡½æ•°ä¹Ÿä¼šè¢«å‘é€åˆ°ä¸åŒrddä¸Š


```python   
val lines = sc.textFile("data.txt")  
val lineLengths = lines.map(s => s.length)  
val totalLength = lineLengths.reduce((a, b) => a + b)
```
> ç¬¬ä¸€è¡Œæ˜¯å®šä¹‰æ¥è‡ªäºŽå¤–éƒ¨æ–‡ä»¶çš„ RDDã€‚è¿™ä¸ªæ•°æ®é›†å¹¶æ²¡æœ‰åŠ è½½åˆ°å†…å­˜æˆ–åšå…¶ä»–çš„æ“ä½œï¼šlinesä»…ä»…æ˜¯ä¸€ä¸ªæŒ‡å‘æ–‡ä»¶çš„æŒ‡é’ˆã€‚ç¬¬äºŒè¡Œæ˜¯å®šä¹‰ lineLengthsï¼Œå®ƒæ˜¯ mapè½¬æ¢(transformation)çš„ç»“æžœã€‚åŒæ ·ï¼ŒlineLengthsç”±äºŽæ‡’æƒ°æ¨¡å¼ä¹Ÿæ²¡æœ‰ç«‹å³è®¡ç®—ã€‚æœ€åŽï¼Œæˆ‘ä»¬æ‰§è¡Œ reduceï¼Œå®ƒæ˜¯ä¸€ä¸ªåŠ¨ä½œ(action)ã€‚åœ¨è¿™ä¸ªåœ°æ–¹ï¼ŒSparkæŠŠè®¡ç®—åˆ†æˆå¤šä¸ªä»»åŠ¡(task)ï¼Œå¹¶ä¸”è®©å®ƒä»¬è¿è¡Œåœ¨å¤šä¸ªæœºå™¨ä¸Šã€‚æ¯å°æœºå™¨éƒ½è¿è¡Œè‡ªå·±çš„ map éƒ¨åˆ†å’Œæœ¬åœ°reduce éƒ¨åˆ†ã€‚ç„¶åŽä»…ä»…å°†ç»“æžœè¿”å›žç»™é©±åŠ¨ç¨‹åºã€‚  



Internal:  
### Partitionï¼š

æŸ¥çœ‹JOBï¼šhttp://127.0.0.1:4040/jobs/job/?id=0  
æ¯ä¸€ä¸ªworkerï¼ˆmachineï¼‰å¯ä»¥åŒ…å«å¤šä¸ªpartitionï¼Œpartitionä¸€èˆ¬æ˜¯å¤„ç†å™¨æ ¸å¿ƒçš„æ•°é‡   
1ã€repartitionç±»çš„æ“ä½œï¼šæ¯”å¦‚repartitionã€repartitionAndSortWithinPartitionsã€coalesceç­‰  
2ã€byKeyç±»çš„æ“ä½œï¼šæ¯”å¦‚reduceByKeyã€groupByKeyã€sortByKeyã€countByKeyç­‰  
3ã€joinç±»çš„æ“ä½œï¼šæ¯”å¦‚joinã€cogroupç­‰  

Sparkæ”¯æŒï¼š  
Two kinds of partitioning available in Spark:  
- Hash partitioning  
- Range partitioning  

æ•°æ®è°ƒç”¨hashCodeå‡½æ•°åˆ†é…æ•°æ®ï¼š  
In general, hash partitioning allocates tuple (k, v) to partition p where 
p = k.hashCode() % numPartitions
Usually works well but be aware of bad inputs!  
ä¾‹å¦‚ï¼š
We see that it hashed all numbers x such that x mod 8 = 1 to partition #1    

def glom(): RDD[Array[T]]

è¯¥å‡½æ•°æ˜¯å°†RDDä¸­æ¯ä¸€ä¸ªåˆ†åŒºä¸­ç±»åž‹ä¸ºTçš„å…ƒç´ è½¬æ¢æˆArray[T]ï¼Œè¿™æ ·æ¯ä¸€ä¸ªåˆ†åŒºå°±åªæœ‰ä¸€ä¸ªæ•°ç»„å…ƒç´    
when the default partition function (hashing) doesnâ€™t work well -ã€‹ a custom partition function  
Specify the partition function in transformations like reduceByKey, groupByKey


Map:è¿™é‡Œæœ‰ä¸€ä¸ªæ€§èƒ½é—®é¢˜ï¼Œmapå› ä¸ºå¯èƒ½ä¼šæ”¹å˜keyï¼Œè€Œpartitionæ˜¯æŒ‰ç…§keyæ¥åˆ†çš„ï¼Œæ‰€ä»¥å°±éœ€è¦æ”¶å›žç„¶åŽå†hash->partitionåˆ†ä¸‹åŽ»æ‰§è¡Œï¼›è€Œmapvaluesåªå˜valueï¼Œæ‰€ä»¥ä¸ç”¨å†partition  
- map(function)   
mapæ˜¯å¯¹RDDä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æ‰§è¡Œä¸€ä¸ªæŒ‡å®šçš„å‡½æ•°æ¥äº§ç”Ÿä¸€ä¸ªæ–°çš„RDDã€‚ä»»ä½•åŽŸRDDä¸­çš„å…ƒç´ åœ¨æ–°RDDä¸­éƒ½æœ‰ä¸”åªæœ‰ä¸€ä¸ªå…ƒç´ ä¸Žä¹‹å¯¹åº”ã€‚  
- mapPartitions(function)   
map()çš„è¾“å…¥å‡½æ•°æ˜¯åº”ç”¨äºŽRDDä¸­æ¯ä¸ªå…ƒç´ ï¼Œè€ŒmapPartitions()çš„è¾“å…¥å‡½æ•°æ˜¯åº”ç”¨äºŽæ¯ä¸ªåˆ†åŒº  
- mapValues(function)   
åŽŸRDDä¸­çš„Keyä¿æŒä¸å˜ï¼Œä¸Žæ–°çš„Valueä¸€èµ·ç»„æˆæ–°çš„RDDä¸­çš„å…ƒç´ ã€‚å› æ­¤ï¼Œè¯¥å‡½æ•°åªé€‚ç”¨äºŽå…ƒç´ ä¸ºKVå¯¹çš„RDD  
- flatMap(function)   
ä¸Žmapç±»ä¼¼ï¼ŒåŒºåˆ«æ˜¯åŽŸRDDä¸­çš„å…ƒç´ ç»mapå¤„ç†åŽåªèƒ½ç”Ÿæˆä¸€ä¸ªå…ƒç´ ï¼Œè€ŒåŽŸRDDä¸­çš„å…ƒç´ ç»flatmapå¤„ç†åŽå¯ç”Ÿæˆå¤šä¸ªå…ƒç´ ï¼Œ***ä¼šæ¶ˆåŽ»ä¸€çº§è¿›è¡Œåˆå¹¶ï¼Œå˜æˆä¸€ä¸ªlist***  

- åˆ†åŒºæ“ä½œ  
ï¼ˆ1ï¼‰mapPartitions
def mapPartitions[U](f: (Iterator[T]) => Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
è¯¥å‡½æ•°å’Œmapå‡½æ•°ç±»ä¼¼ï¼Œåªä¸è¿‡æ˜ å°„å‡½æ•°çš„å‚æ•°ç”±RDDä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ å˜æˆäº†RDDä¸­æ¯ä¸€ä¸ªåˆ†åŒºçš„è¿­ä»£å™¨ã€‚å¦‚æžœåœ¨æ˜ å°„çš„è¿‡ç¨‹ä¸­éœ€è¦é¢‘ç¹åˆ›å»ºé¢å¤–çš„å¯¹è±¡ï¼ˆå¦‚æ•°æ®åº“è¿žæŽ¥å¯¹è±¡ï¼‰ï¼Œä½¿ç”¨mapPartitionsè¦æ¯”mapé«˜æ•ˆçš„å¤šã€‚
æ¯”å¦‚ï¼Œå°†RDDä¸­çš„æ‰€æœ‰æ•°æ®é€šè¿‡JDBCè¿žæŽ¥å†™å…¥æ•°æ®åº“ï¼Œå¦‚æžœä½¿ç”¨mapå‡½æ•°ï¼Œå¯èƒ½è¦ä¸ºæ¯ä¸€ä¸ªå…ƒç´ éƒ½åˆ›å»ºä¸€ä¸ªconnectionï¼Œè¿™æ ·å¼€é”€å¾ˆå¤§ï¼Œå¦‚æžœä½¿ç”¨mapPartitionsï¼Œé‚£ä¹ˆåªéœ€è¦é’ˆå¯¹æ¯ä¸€ä¸ªåˆ†åŒºå»ºç«‹ä¸€ä¸ªconnectionã€‚

ä¾èµ–é—®é¢˜ï¼š
wide dependenciesï¼šä¸åœ¨ä¸€ä¸ªpartitionä¸Šé¢ï¼Œéœ€è¦shuffleï¼Ÿ
narrowï¼›ï¼›

shuffle: æå‰åšå¥½å¹³è¡¡ï¼Œè´¹èµ„æº
repartition: same tumple column is in the same partition


Jobï¼šçœŸæ­£çš„è¿è¡Œèµ·æ¥åŠ¨ä½œ
å¾ˆå¤šä¸ªstageç»„æˆä¸€ä¸ªjob,æ¯ä¸€ä¸ªthreadå¯ä»¥è¿è¡Œä¸€ä¸ªjobè¾¾åˆ°å¹¶è¡Œçš„æ•ˆæžœ

ä¸€ä¸ªRDD åªèƒ½æ˜¯ä¸€ä¸ªkey-value pairï¼Œå¯ä»¥è¢«åˆ†æˆå¤šä¸ªpartitionï¼Œä¸€èˆ¬æ¥è¯´æ¯ä¸ªworker(ä¸€èˆ¬æ˜¯core çš„æ•°é‡ï¼Œå³JVM)è¿è¡Œä¸¤ä¸ªï¼Œä½†æ˜¯sparkä¼šæŠŠæ…¢çš„partitionä¼ åˆ°å¿«çš„workerä¸Šè¿è¡Œ

## æ•°æ®å€¾æ–œ
- ä¸ºskewçš„keyå¢žåŠ éšæœºå‰/åŽç¼€  
åŽŸç†
ä¸ºæ•°æ®é‡ç‰¹åˆ«å¤§çš„Keyå¢žåŠ éšæœºå‰/åŽç¼€ï¼Œä½¿å¾—åŽŸæ¥Keyç›¸åŒçš„æ•°æ®å˜ä¸ºKeyä¸ç›¸åŒçš„æ•°æ®ï¼Œä»Žè€Œä½¿å€¾æ–œçš„æ•°æ®é›†åˆ†æ•£åˆ°ä¸åŒçš„Taskä¸­ï¼Œå½»åº•è§£å†³æ•°æ®å€¾æ–œé—®é¢˜ã€‚Joinå¦ä¸€åˆ™çš„æ•°æ®ä¸­ï¼Œä¸Žå€¾æ–œKeyå¯¹åº”çš„éƒ¨åˆ†æ•°æ®ï¼Œä¸Žéšæœºå‰ç¼€é›†ä½œç¬›å¡å°”ä¹˜ç§¯ï¼Œä»Žè€Œä¿è¯æ— è®ºæ•°æ®å€¾æ–œä¾§å€¾æ–œKeyå¦‚ä½•åŠ å‰ç¼€ï¼Œéƒ½èƒ½ä¸Žä¹‹æ­£å¸¸Joinã€‚
- å°†Reduce side Joinè½¬å˜ä¸ºMap side Join  
- è‡ªå®šä¹‰Partitioner  
åŽŸç†
- ä½¿ç”¨è‡ªå®šä¹‰çš„Partitionerï¼ˆé»˜è®¤ä¸ºHashPartitionerï¼‰ï¼Œå°†åŽŸæœ¬è¢«åˆ†é…åˆ°åŒä¸€ä¸ªTaskçš„ä¸åŒKeyåˆ†é…åˆ°ä¸åŒTaskã€‚  
- ä½¿ç”¨Partitionerå¿…é¡»æ»¡è¶³ä¸¤ä¸ªå‰æï¼Œ1ã€RDDæ˜¯k-vå½¢å¼ï¼Œå¦‚RDD[(K, V)]ï¼Œ2ã€æœ‰shuffleæ“ä½œã€‚å¸¸è§çš„è§¦å‘shuffleçš„æ“ä½œæœ‰ï¼š 
    1.combineByKey(groupByKey, reduceByKey , aggregateByKey) 
    2. sortByKey 
    3. join(leftOuterJoin, rightOuterJoin, fullOuterJoin) 
    4. cogroup 
    5. repartition(coalesce(shuffle=true)) 
    6. groupWith 
    7. repartitionAndSortWithinPartitions


## Classical Divide-and-Conquer:  

åªæœ‰å…¨éƒ¨çš„ç»“æžœï¼Œæ²¡æœ‰ä¸­é—´çš„æˆ–è€…prefix resultï¼ˆå‰pä¸ªåŽŸå…ƒç´ çš„å’Œï¼‰  
è®¡ç®—prefixçš„parallelï¼šé’ˆå¯¹åºåˆ—è®¡ç®—ï¼Œå¦‚ä½•è¿›è¡Œå¹¶è¡ŒåŒ–
Algorithm:
- Compute sum for each partition æ¯ä¸ªpartitionçš„å°sum
- Compute the prefix sums of the ð‘ 
- Compute prefix sums in each partition

x = [1, 4, 3, 5, 6, 7, 0, 1]  
sum = [5, 8, 13, 1] mapPartitions
prefix_sum = [1, 5, 8, 13, 19, 26, 26, 27]  mapPartitionsWithIndex  

### å…±äº«å˜é‡
sparkå°†å‡½æ•°å’Œå˜é‡çš„ç‹¬ç«‹å‰¯æœ¬ä¼ é€’ç»™workersï¼Œæ›´æ–°ä¸ä¼šè¿”å›žç»™é©±åŠ¨ç¨‹åºï¼Œæ˜¯çš„è·¨ä»»åŠ¡è¯»å†™é€Ÿåº¦å¾ˆæ…¢
- å¹¿æ’­å˜é‡ï¼ˆbroadcast variableï¼‰
SparkContext.broadcast(v)æ–¹æ³•ä»Žä¸€ä¸ªåˆå§‹å˜é‡vä¸­åˆ›å»ºã€‚å¹¿æ’­å˜é‡æ˜¯vçš„ä¸€ä¸ªåŒ…è£…å˜é‡ï¼Œå®ƒçš„å€¼å¯ä»¥é€šè¿‡valueæ–¹æ³•è®¿é—®
- ç´¯åŠ å™¨ï¼ˆaccumulatorï¼‰
é€šè¿‡è°ƒç”¨SparkContext.accumulator(v)æ–¹æ³•ä»Žä¸€ä¸ªåˆå§‹å˜é‡vä¸­åˆ›å»ºã€‚è¿è¡Œåœ¨é›†ç¾¤ä¸Šçš„ä»»åŠ¡å¯ä»¥é€šè¿‡addæ–¹æ³•æˆ–è€…ä½¿ç”¨+=æ“ä½œæ¥ç»™å®ƒåŠ å€¼ã€‚ç„¶è€Œï¼Œå®ƒä»¬æ— æ³•è¯»å–è¿™ä¸ªå€¼ã€‚åªæœ‰é©±åŠ¨ç¨‹åºå¯ä»¥ä½¿ç”¨valueæ–¹æ³•æ¥è¯»å–ç´¯åŠ å™¨çš„å€¼

# set

setæœ‰ä¸¤ç§ç±»åž‹ï¼Œsetå’Œfrozensetã€‚

setæ˜¯å¯å˜çš„ï¼Œæœ‰addï¼ˆï¼‰ï¼Œremoveï¼ˆï¼‰ç­‰æ–¹æ³•ã€‚æ—¢ç„¶æ˜¯å¯å˜çš„ï¼Œæ‰€ä»¥å®ƒä¸å­˜åœ¨å“ˆå¸Œå€¼ã€‚

frozensetæ˜¯å†»ç»“çš„é›†åˆï¼Œå®ƒæ˜¯ä¸å¯å˜çš„ï¼Œå­˜åœ¨å“ˆå¸Œå€¼ï¼Œå¥½å¤„æ˜¯å®ƒå¯ä»¥ä½œä¸ºå­—å…¸çš„keyï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå…¶å®ƒé›†åˆçš„å…ƒç´ ã€‚ç¼ºç‚¹æ˜¯ä¸€æ—¦åˆ›å»ºä¾¿ä¸èƒ½æ›´æ”¹ï¼Œæ²¡æœ‰addï¼Œremoveæ–¹æ³•ã€‚


[img](http://dongguo.me/images/personal/engineering/spark/spark-components.png)


### æ­£ç¡®åœ°ä½¿ç”¨å¹¿æ’­å˜é‡(broadcast variables)
- å¹¿æ’­å˜é‡å…è®¸ç¨‹åºå‘˜å°†ä¸€ä¸ªåªè¯»çš„å˜é‡ç¼“å­˜åœ¨æ¯å°æœºå™¨ä¸Šï¼Œè€Œä¸ç”¨åœ¨ä»»åŠ¡ä¹‹é—´ä¼ é€’å˜é‡ã€‚å¹¿æ’­å˜é‡å¯è¢«ç”¨äºŽæœ‰æ•ˆåœ°ç»™æ¯ä¸ªèŠ‚ç‚¹ä¸€ä¸ªå¤§è¾“å…¥æ•°æ®é›†çš„å‰¯æœ¬ã€‚
ä¸€ä¸ªExecutoråªéœ€è¦åœ¨ç¬¬ä¸€ä¸ªTaskå¯åŠ¨æ—¶ï¼ŒèŽ·å¾—ä¸€ä»½Broadcastæ•°æ®ï¼Œä¹‹åŽçš„Taskéƒ½ä»Žæœ¬èŠ‚ç‚¹çš„BlockManagerä¸­èŽ·å–ç›¸å…³æ•°æ®ã€‚

- å¦‚æžœæˆ‘ä»¬æœ‰ä¸€ä»½constæ•°æ®ï¼Œéœ€è¦åœ¨executorsä¸Šç”¨åˆ°ï¼Œä¸€ä¸ªå…¸åž‹çš„ä¾‹å­æ˜¯Driverä»Žæ•°æ®åº“ä¸­loadäº†ä¸€ä»½æ•°æ®dbDataï¼Œåœ¨å¾ˆå¤šRDDæ“ä½œä¸­éƒ½å¼•ç”¨äº†dbDataï¼Œè¿™æ ·çš„è¯ï¼Œæ¯æ¬¡RDDæ“ä½œï¼Œdriver nodeéƒ½éœ€è¦å°†dbDataåˆ†å‘åˆ°å„ä¸ªexecutors nodeä¸€éï¼ˆåˆ†äº«1ä¸­å·²ç»ä»‹ç»äº†èƒŒæ™¯ï¼‰ï¼Œè¿™éžå¸¸çš„ä½Žæ•ˆï¼Œç‰¹åˆ«æ˜¯dbDataæ¯”è¾ƒå¤§ä¸”RDDæ“ä½œæ¬¡æ•°è¾ƒå¤šæ—¶ã€‚Sparkçš„å¹¿æ’­å˜é‡ä½¿å¾—Driverå¯ä»¥æå‰åªç»™å„ä¸ªexecutors nodeä¼ ä¸€éï¼ˆsparkå†…éƒ¨å…·ä½“çš„å®žçŽ°å¯èƒ½æ˜¯driverä¼ ç»™æŸå‡ ä¸ªexecutorsï¼Œè¿™å‡ ä¸ªexecutorså†ä¼ ç»™å…¶ä½™executorsï¼‰ã€‚ä½¿ç”¨å¹¿æ’­å˜é‡æœ‰ä¸€ä¸ªæˆ‘çŠ¯è¿‡çš„é”™è¯¯å¦‚ä¸‹ï¼š

``` javascript
 val brDbData = sparkContext.broadcast(dbData) //broadcast dbDataA, and name it as brDbData
 val dbDataB = brDbData.value //no longer broadcast variable
 oneRDD.map(x=>{dbDataB.getOrElse(key, -1); â€¦})
 ```
ç¬¬ä¸€è¡Œå°†dbDataå·²ç»å¹¿æ’­å‡ºåŽ»ä¸”å‘½åä¸ºbrDbDataï¼Œä¸€å®šè¦åœ¨RDDæ“ä½œä¸­ç›´æŽ¥ä½¿ç”¨è¯¥å¹¿æ’­å˜é‡ï¼Œå¦‚æžœæå‰æå–å‡ºå€¼ï¼Œç¬¬ä¸‰è¡Œçš„RDDæ“ä½œè¿˜éœ€è¦å°†dbDataä¼ é€ä¸€éã€‚æ­£ç¡®çš„ä»£ç å¦‚ä¸‹

 ``` javascript
 val brDbData = sparkContext.broadcast(dbData) //broadcast dbDataA, and name it as brDbData
 oneRDD.map(x=>{brDbData.value.getOrElse(key, -1); â€¦})
 ```
Sparkæœ¬èº«æ”¯æŒåšbatchçš„è®¡ç®—ï¼Œæ¯”å¦‚æ¯å¤©æœºå™¨å­¦ä¹ æ¨¡åž‹çš„è®­ç»ƒï¼Œå„ç§æ•°æ®çš„å¤„ç†ï¼›
Spark Streamingå¯ä»¥ç”¨æ¥åšrealtimeè®¡ç®—å’Œæ•°æ®å¤„ç†ï¼ŒSpark Streamingçš„APIå’ŒSparkçš„æ¯”è¾ƒç±»ä¼¼ï¼Œå…¶å®žèƒŒåŽçš„å®žçŽ°ä¹Ÿæ˜¯æŠŠä¸€æ®µæ®µçš„realtimeæ•°æ®ç”¨batchçš„æ–¹å¼åŽ»å¤„ç†ï¼›
MLlibå®žçŽ°äº†å¸¸ç”¨çš„æœºå™¨å­¦ä¹ å’ŒæŽ¨èç®—æ³•ï¼Œå¯ä»¥ç›´æŽ¥ç”¨æˆ–è€…ä½œä¸ºbaselineï¼›
Spark SQLä½¿å¾—å¯ä»¥é€šè¿‡SQLæ¥å¯¹Hiveè¡¨ï¼ŒJsonæ–‡ä»¶ç­‰æ•°æ®æºè¿›è¡ŒæŸ¥è¯¢ï¼ŒæŸ¥è¯¢ä¼šè¢«è½¬å˜ä¸ºä¸€ä¸ªSpark jobï¼›

- ä¸€èˆ¬å¯ä»¥è®¾ç½®taskçš„æ•°é‡æ˜¯coreçš„2-3å€ï¼Œè®©CPUä¸ç©ºé—²
- 
### stream  
- streamingContext.start()  
Start receiving data and processing it using 
- streamingContext.awaitTermination()  
Wait for the processing to be stopped 
- streamingContext.stop()  
(manually or due to any error) using The processing can be manually stopped using streamingContext.stop().  
. To stop only the StreamingContextï¼š ssc.stop(False)
- DStreams can be created   
    - from input data streams 
    - by applying high-level operations on other DStreams.
#### Streaming MG analysis


### Hbase table åˆ—å­˜å‚¨
é«˜éšæœºè¯»å†™ä¸æ”¯æŒjoinï¼Œ
An open-source version of Google BigTable  
HBase is a distributed column-oriented data store built on top of HDFS  
HBase is an Apache open source project whose goal is to provide storage for Hadoop Distributed Computing   
Data is logically organized into tables, rows and columns 
- Each row has a Key
- Each record is divided into Column Families
- Each column family consists of one or more Columns
- HBase schema consists of several Tables  
    Each table consists of a set of Column Families  
    Columns are not part of the schema 
#### Hive: data warehousing application in Hadoop ç±»ä¼¼sql  
Query language is HQL, variant of SQL  
Tables stored on HDFS as flat files  
Developed by Facebook, now open source  
Hive looks similar to an SQL database
Support Joins

### å‡½æ•°ï¼š
glomï¼š  
å°†RDDä¸­æ¯ä¸€ä¸ªåˆ†åŒºä¸­ç±»åž‹ä¸ºTçš„å…ƒç´ è½¬æ¢æˆArray[T]ï¼Œè¿™æ ·æ¯ä¸€ä¸ªåˆ†åŒºå°±åªæœ‰ä¸€ä¸ªæ•°ç»„å…ƒç´ ã€‚


Platform as a Service: å¹³å°å³æœåŠ¡, æ˜¯é¢å‘è½¯ä»¶å¼€å‘è€…çš„æœåŠ¡, äº‘è®¡ç®—å¹³å°æä¾›ç¡¬ä»¶,
SaaS: è½¯ä»¶å³æœåŠ¡, æ˜¯é¢å‘è½¯ä»¶æ¶ˆè´¹è€…çš„

# å¸¸ç”¨ä»£ç å’Œé—®é¢˜


```python 
# åœ¨Notebookä¸­è¿è¡Œspark
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc = SparkContext()
spark = SparkSession(sc)

# åˆ†åŒºä¸¤ç§æ–¹å¼  å¯ä»¥ç”¨ partitionBy(4,lambda x:x%4)
rdd = sc.parallelize([('a', 1), ('a', 2), ('b', 1), ('b', 3), ('c',1), ('ef',5)])
rdd1 = rdd.repartition(4)
rdd1.glom().collect()

# reduceByKey ç›¸åŒçš„keyç›¸åŠ ï¼ŒFalseæ˜¯é™åº
 pairs.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1], False).zipWithIndex().map(lambda x:(x[1],x[0]))
 
 #graph æ“ä½œ
 rule12 = g.find('(a)-[]->(b)').filter('a.id == "h"').select('b.name')
 
 # DFæ“ä½œ  .toDF()
TotalPrice = dfDetail.select('*',(dfDetail.UnitPrice*dfDetail.OrderQty*(1-dfDetail.UnitPriceDiscount)).alias('netprice'))\
            .groupBy('SalesOrderID').sum('netprice')\
                .withColumnRenamed('sum(netprice)','TotalPrice')

```

```python

#!/usr/bin/python
# -*- coding: UTF-8 -*-
from __future__ import division
import csv
import random
from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark.sql.functions import *
import time


def genTransactions(inpath):
    """ Generates a list of transactions, where each transaction is a set of items. 
    """
    ids, ts = list(), list()
    with open(inpath, 'r') as f:
        for line in f:
            i, t = line.strip().split('\t')
            ids.append(i)
            ts.append(set(t.split(',')))
    return ids, ts


def prune(curr, F):
    for i in curr:
        tmp = curr - {i}
        if not tmp in F: return False
    return True


def genCand(F):
    C = set()
    s = len(F[0]) + 1
    for i in xrange(len(F)-1): 
        for j in xrange(i+1, len(F)):
            curr = F[i].union(F[j])
            if len(curr) == s:
                C.add(curr) # tuple: (frozenset(shared_elems), new_elem)
    return list(C)


def getSupport(x, n, values):
    c = 0
    for t in values:
        if x.issubset(t):
            c += 1
    return (frozenset(x), c/n)


def check(x, values):
    for v in values:
        if x.issubset(v):
            return (frozenset([x]),1)
    return (frozenset([x]),0)



def genToyTransactions():
    """ Generates a list of transactions, where each transaction is a set of items. 
    """
    T = list()
    T.append({1,3,4})
    T.append({2,3,5})
    T.append({1,2,3,5})
    T.append({2,5})
    return T


sc = SparkContext()

def apriori(T,minsup,niters):

    n = float(len(T))
    results = dict() #results[k] saves k-item frequent set

    trans = sc.parallelize(T,NUM_PARTITIONS)
    trans_set = trans.map(lambda x: frozenset(x)).cache()

    # Gets one-item candidate set
    trans_set_one = trans.flatMap(lambda x: x)
    unique_one = trans_set_one.map(lambda x: (frozenset([x]),1)).reduceByKey(lambda x,y:x+y)

    # Gets one-item frequent set
    freq_set_values = unique_one.filter(lambda x: x[1]/n>=minsup)
    freq_set_one = freq_set_values.map(lambda x:x[0]).collect()
    trans_set_bc = sc.broadcast(trans_set.collect())

    c = 1
    results[c] = freq_set_one
    while c < niters+1:
        if len(results[c]) == 0:
            print c
            break
        print ('== %s th iteration ==' % str(c))
        t0 = time.time()
        cand = genCand(results[c])
        print 'join set time = ', time.time() - t0
        candidates = sc.parallelize(cand,NUM_PARTITIONS)
        # result_c_bc = sc.broadcast(results[c])
        # cand_set_pruned = candidates.filter(lambda x: prune(x,result_c_bc.value)).cache()
        cand_set_pruned = candidates.filter(lambda x: prune(x,results[c])).cache()

        t0 = time.time()
        freq_set = cand_set_pruned\
                               .map(lambda x: getSupport(x, n, trans_set_bc.value))\
                                                   .filter(lambda x:x[1]>=minsup)\
                                                                .map(lambda x: x[0]).collect()
        # freq_set = cand_set_pruned.map(lambda x: check(x, trans_set_bc.value)).reduceByKey(lambda x,y:x+y).filter(lambda x:x[1]/n>=minsup).map(lambda x: x[0]).collect()

        print 'freq time = ', time.time()-t0

        c += 1
        results[c] = freq_set
    return results 

NUM_PARTITIONS = 1
if __name__ == '__main__':
    
    minsup = 0.001 #minimum support ratio
    niters = 2 #number of iterations
    maxlength = 5
    

    inpath = 'data1/transactions.txt'
    ids, T = genTransactions(inpath)
    del ids
    newT = list()
    for i in range(len(T)):
        if len(T[i]) < maxlength:
            newT.append(T[i])
    T = newT
    

    # T = genToyTransactions()

    print '\nNUM PARTITIONS: ', NUM_PARTITIONS
    print 'MUN TRANSACTIONS = ', len(T)
    print 'MINSUP = ', minsup
    print 

    t_start = time.time()
    results = apriori(T,minsup,niters)
    t_end = time.time()
    # print results 
    print 'TOTAL TIME USED = ', t_end-t_start

    # print results




```
