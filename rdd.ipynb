{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Notebook setup\n",
    "\n",
    "When using PySpark kernel notebooks on HDInsight, there is no need to create a SparkContext or a SparkSession; a SparkSession which has the SparkContext is created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkSession (spark)\n",
    "- SparkContext (sc)\n",
    "\n",
    "To run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do I make an RDD?\n",
    "\n",
    "RDDs can be created from stable storage or by transforming other RDDs. Run the cells below to create RDDs from the sample data files available in the storage container associated with your Spark cluster. One such sample data file is available on the cluster at `wasb:///example/data/fruits.txt`.  The /// notation reads data from the default container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(xrange(1, 100))\n",
    "t = 50\n",
    "B = A.filter(lambda x: x < t)\n",
    "# print B.cache()\n",
    "print B.count()\n",
    "t = 10\n",
    "C = B.filter(lambda x: x > t)\n",
    "print C.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = sc.textFile('wasb:///example/data/fruits.txt')\n",
    "yellowThings = sc.textFile('wasb:///example/data/yellowthings.txt')\n",
    "fruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In local mode:\n",
    "fruits = sc.textFile('../data/fruits.txt')\n",
    "yellowThings = sc.textFile('../data/yellowthings.txt')\n",
    "print fruits.collect()\n",
    "print yellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also read from other containers.\n",
    "# The 'cluster' container under the storage account 'msbd' has been made public.\n",
    "# Use the following format to read data from a public container\n",
    "# The file can also be accessed from the web at: \n",
    "# https://msbd.blob.core.windows.net/cluster/data/course.txt\n",
    "\n",
    "txtfile = sc.textFile('wasb://cluster@msbd.blob.core.windows.net/data/course.txt')\n",
    "txtfile.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "##  RDD operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map 倒序\n",
    "fruitsReversed = fruits.map(lambda fruit: fruit[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruitsReversed.unpersist()\n",
    "# try changing the file and re-execute with and without cache\n",
    "fruitsReversed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter\n",
    "shortFruits = fruits.filter(lambda fruit: len(fruit) <= 5)\n",
    "shortFruits.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在使用时map会将一个长度为N的RDD转换为另一个长度为N的RDD；而flatMap会将一个长度为N的RDD转换成一个N个元素的集合，然后再把这N个元素合成到一个单个RDD的结果集  \n",
    "flatMap相对于map多了的是[[“a”,”b”,”c”],[],[“d”]] => [“a”,”b”,”c”,”d”]这一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flatMap\n",
    "characters = fruits.flatMap(lambda fruit: list(fruit))\n",
    "characters.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# union\n",
    "fruitsAndYellowThings = fruits.union(yellowThings)\n",
    "fruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# intersection\n",
    "yellowFruits = fruits.intersection(yellowThings)\n",
    "yellowFruits.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distinct\n",
    "distinctFruitsAndYellowThings = fruitsAndYellowThings.distinct()\n",
    "distinctFruitsAndYellowThings.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD actions\n",
    "Following are examples of some of the common actions available. For a detailed list, see [RDD Actions](https://spark.apache.org/docs/2.0.0/programming-guide.html#actions).\n",
    "\n",
    "Run some transformations below to understand this better. Place the cursor in the cell and press **SHIFT + ENTER**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect\n",
    "fruitsArray = fruits.collect()\n",
    "yellowThingsArray = yellowThings.collect()\n",
    "fruitsArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count\n",
    "numFruits = fruits.count()\n",
    "numFruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take\n",
    "first3Fruits = fruits.take(3)\n",
    "first3Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduce\n",
    "letterSet = fruits.map(lambda fruit: set(fruit)).reduce(lambda x, y: x.union(y))\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letterSet = fruits.flatMap(lambda fruit: list(fruit)).distinct().collect()\n",
    "letterSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closure 闭包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 广播变量（broadcast variables）：将一个只读变量缓存到集群的每个节点上。例如，将一份数据的只读缓存分发到每个节点。\n",
    "- 累加变量（accumulators）：只允许add操作，用于计数、求和  \n",
    "- 在驱动程序中，在一个已经存在的集合上调用SparkContext的parallelize方法可以创建一个并行集合。集合里的元素将被复制到一个可被并行操作的分布式数据集中  \n",
    "\n",
    "共享变量  \n",
    "一般来说，当一个函数被传递给Spark操作（例如map和reduce），在一个远程集群上运行，它实际上操作的是这个函数用到的所有变量的独立拷贝。这些变量会被拷贝到每一台机器，在远程机器上对变量的所有更新都不会被传播回驱动程序。通常看来，在任务之间中，读写共享变量显然不够高效。然而，Spark还是为两种常见的使用模式，提供了两种有限的共享变量：广播变量和累加器。  \n",
    "\n",
    "- 广播变量 Broadcast Variables\n",
    "广播变量允许程序员保留一个只读的变量，缓存在每一台机器上，而非每个任务保存一份拷贝。他们可以这样被使用，例如，以一种高效的方式给每个结点一个大的输入数据集。Spark会尝试使用一种高效的广播算法来传播广播变量，从而减少通信的代价。  \n",
    "\n",
    "广播变量是通过调用SparkContext.broadcast(v)方法从变量v创建的。广播变量是一个v的封装器，它的值可以通过调用value方法获得。\n",
    "\n",
    "- 加器是一种只能通过关联操作进行“加”操作的变量，因此可以高效被并行支持。它们可以用来实现计数器（如MapReduce中）和求和器。Spark原生就支持Int和Double类型的累加器，开发者可以自己添加新的支持类型。  \n",
    "\n",
    "一个累加器可以通过调用SparkContext.accumulator(v)方法从一个初始值v中创建。运行在集群上的任务，可以通过使用+=来给它加值。然而，他们不能读取这个值。只有驱动程序可以使用value的方法来读取累加器的值。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[5, 6, 7, 8, 9]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(xrange(5,10))\n",
    "\n",
    "# Wrong: Don't do this!! ##？\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "#     return counter\n",
    "rdd.foreach(increment_counter)\n",
    "print rdd.count()\n",
    "print rdd.collect()\n",
    "print counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[68] at RDD at PythonRDD.scala:48\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(xrange(10))\n",
    "print(rdd)\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "#     accum.add(x)\n",
    "\n",
    "a = rdd.foreach(g)\n",
    "\n",
    "print accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在分布式下运行时，建议使用累加器定义一些全局集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，发送给每个executor的闭包内的变量是当前变量的副本，因此当counter在foreach中被引用时，已经不是在driver节点上的counter了。在driver节点的内存中仍然有一个counter，但这个counter对executors不可见。executor只能操作序列化的闭包中的counter副本。因此，最终counter的值仍然是0，因为所有对counter的操作都是在序列化的闭包内的counter上进行的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def f(x):\n",
    "    accum.add(x)\n",
    "sc.parallelize([1,2,3,4]).foreach(lambda x:f(x))\n",
    "accum.value# executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def f(x):\n",
    "    accum.add(x)\n",
    "print sc.parallelize([1,2,3,4]).map(lambda x:f(x)).collect()\n",
    "accum.value# not execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x,1)\n",
    "rdd.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Accumulator<id=15, value=0>, Accumulator<id=15, value=0>, Accumulator<id=15, value=0>, Accumulator<id=15, value=0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def f(x):\n",
    "    accum.add(x)\n",
    "    return accum\n",
    "print sc.parallelize([1,2,3,4]).map(lambda x:f(x)).reduce\n",
    "accum.value# not execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "45\n",
      "45\n",
      "45\n",
      "90\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(xrange(10))\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    global accum\n",
    "    accum += x\n",
    "    return x * x\n",
    "\n",
    "a = rdd.map(g) #还未执行，\n",
    "print accum.value\n",
    "print rdd.reduce(lambda x, y: x+y)\n",
    "# a.cache() # 后面的a会变成45，不再对global进行操作\n",
    "tmp = a.count() #accum的值才更新\n",
    "print accum.value\n",
    "print rdd.reduce(lambda x, y: x+y)\n",
    "\n",
    "tmp = a.count()\n",
    "print accum.value\n",
    "print rdd.reduce(lambda x, y: x+y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "rdd = sc.parallelize(xrange(10))\n",
    "print rdd.collect()\n",
    "print rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 2, 3, 4]\n",
      "[0, 1, 2, 3, 4]\n",
      "3\n",
      "[0, 1, 2]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "A = sc.parallelize(xrange(10))\n",
    "\n",
    "print A.collect()\n",
    "\n",
    "x = 5\n",
    "B = A.filter(lambda z: z < x) #更改后x会变成5 \n",
    "#B.cache()\n",
    "print B.take(10)\n",
    "print B.collect()\n",
    "x = 3 # 发送给所有的workers\n",
    "print B.count()\n",
    "print B.take(10) \n",
    "print B.collect() #bug? 返回最近的结果，和take不一样\n",
    "#collect() doesn't always re-collect data - bad design!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD variables are references\n",
    "    A = sc.parallelize(xrange(10))\n",
    "B = A.map(lambda x: x*2)\n",
    "A = B.map(lambda x: x+1)\n",
    "A.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 67, 21, 56, 47, 89, 12, 44, 74, 43]\n",
      "left mid,k: 3 4\n",
      "[89, 74]\n",
      "left mid,k: 0 0\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "# Linear-time selection\n",
    "\n",
    "data = [34, 67, 21, 56, 47, 89, 12, 44, 74, 43, 26]\n",
    "A = sc.parallelize(data,2)\n",
    "k = 4\n",
    "# x = A.first()\n",
    "while True:\n",
    "    x = A.first()\n",
    "    A1 = A.filter(lambda z: z < x) # 第二次执行的时候，从最初再开始计算这里的A(来自于上一步的A2)，A会相应的改变，不是上一次应该的A2\n",
    "    A2 = A.filter(lambda z: z > x)\n",
    "    print A.take(10)\n",
    "    mid = A1.count()\n",
    "    print 'left mid,k:',mid,k\n",
    "    if mid == k:\n",
    "        print x\n",
    "        break\n",
    "    if k < mid: # k在左半边\n",
    "        A = A1\n",
    "    else:\n",
    "        A = A2\n",
    "        k = k - mid - 1# 右边，新的A2，重新定义K\n",
    "#     A.cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Pi using Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 蒙特卡罗方法-概率统计计算面积  \n",
    "- 正方形内部有一个相切的圆，它们的面积之比是π/4,现在，在这个正方形内部，随机产生10000个点（即10000个坐标对 (x, y)），计算它们与中心点的距离，从而判断是否落在圆的内部,如果这些点均匀分布，那么圆内的点应该占到所有点的 π/4，因此将这个比值乘以4，就是π的值。通过R语言脚本随机模拟30000个点，π的估算值与真实值相差0.07%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7858300\n",
      "Pi is roughly 3.14332\n"
     ]
    }
   ],
   "source": [
    "# From the official spark examples.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "\n",
    "partitions = 10 # also workers\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    return 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "count = sc.parallelize(xrange(1, n + 1), partitions) \\\n",
    "          .map(f).reduce(lambda a,b: a+b) #gxrange - enerator\n",
    "print(count)\n",
    "\n",
    "print \"Pi is roughly\", 4.0 * count / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9957222540657606"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random.random()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 所有workers的random相同，种子相同，精度相同，要用不同的seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize(xrange(0,20),4)\n",
    "print a.collect()\n",
    "print a.glom().collect()\n",
    "# a.map(lambda x: random.random()).glom().collect() # 所有的workers做相同的工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mapPartitionsWithIndex(分区的idnex，迭代次数) 像函数传递两个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.14152072\n"
     ]
    }
   ],
   "source": [
    "# Correct version\n",
    "\n",
    "partitions = 1000\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(index, it):\n",
    "    random.seed(index + 987236)\n",
    "    for i in it:\n",
    "        x = random.random()\n",
    "        y = random.random()\n",
    "        yield 1 if x ** 2 + y ** 2 < 1 else 0\n",
    "\n",
    "count = sc.parallelize(xrange(1, n + 1), partitions) \\\n",
    "          .mapPartitionsWithIndex(f).reduce(lambda a,b: a+b) ##？\n",
    "\n",
    "print \"Pi is roughly\", 4.0 * count / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key-Value Pairs  \n",
    "- useful operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "numFruitsByLength = fruits.map(lambda fruit: (len(fruit), 1)).reduceByKey(lambda x, y: x + y)\n",
    "numFruitsByLength.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 10),\n",
       " (u'                sum = sum + 1', 3),\n",
       " (u'        number = arrays[x]', 2),\n",
       " (u'import sys', 2),\n",
       " (u'def identify(a, b):', 2),\n",
       " (u'    _b = b', 2),\n",
       " (u'        count = count + 1', 2),\n",
       " (u'            if x == y:', 2),\n",
       " (u'    number = a * 10 ** count + _b', 2),\n",
       " (u'    else:', 2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "lines = sc.textFile('/Users/lucas/Desktop/workspace/temp')\n",
    "counts = lines.flatMap(lambda x: x.split('\\n')) \\\n",
    "              .map(lambda x: (x, 1)) \\\n",
    "              .reduceByKey(add).sortBy(lambda x:x[1],False).take(10)\n",
    "counts\n",
    "# counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t = 50\n",
    "A = sc.parallelize(range(1,100))\n",
    "B = A.filter(lambda x:x<t)\n",
    "B.cache()\n",
    "t = 10\n",
    "C = B.filter(lambda x:x>t).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey  \n",
    "counts.map(lambda (k,v): (v,k)).sortByKey(False).take(20)  \n",
    "### sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', 10),\n",
       " (u'                sum = sum + 1', 3),\n",
       " (u'        number = arrays[x]', 2),\n",
       " (u'import sys', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.sortBy(lambda x: x[1], False).take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 笛卡尔积，所有key相同的pair n*m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI  \n",
    "- tuple() 更适合key-value 操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('',8)\n",
    "lines.count()\n",
    "lines.\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split(' ')])\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex # 返回最近的center的index当做label\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_data.txt\n",
    "lines = sc.textFile('../data/kmeans_data.txt', 5)  \n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/kmeans_bigdata.txt\n",
    "# lines = sc.textFile('../data/kmeans_bigdata.txt', 5)  \n",
    "# lines is an RDD of strings\n",
    "K = 3\n",
    "convergeDist = 0.01 # 终止阈值\n",
    "# terminate algorithm when the total distance from old center to new centers is less than this value\n",
    "\n",
    "data = lines.map(parseVector).cache() # data is an RDD of arrays\n",
    "\n",
    "kCenters = data.takeSample(False, K, 1)  # intial centers as a list of arrays\n",
    "tempDist = 1.0  # total distance from old centers to new centers\n",
    "\n",
    "while tempDist > convergeDist:\n",
    "    closest = data.map(lambda p: (closestPoint(p, kCenters), (p, 1)))#用tuple来存储数据\n",
    "    # for each point in data, find its closest center\n",
    "    # closest is an RDD of tuples (index of closest center, (point, 1))\n",
    "        \n",
    "    pointStats = closest.reduceByKey(lambda p1, p2: (p1[0] + p2[0], p1[1] + p2[1]))\n",
    "    # pointStats is an RDD of tuples (index of center,(array of sums of coordinates, total number of points assigned))\n",
    "    \n",
    "    newCenters = pointStats.map(lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "    # compute the new centers\n",
    "    \n",
    "    tempDist = sum(np.sum((kCenters[i] - p) ** 2) for (i, p) in newCenters)\n",
    "    # compute the total disctance from old centers to new centers\n",
    "    \n",
    "    for (i, p) in newCenters:\n",
    "        kCenters[i] = p\n",
    "        \n",
    "print \"Final centers: \", kCenters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    # Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = urls.split(' ')\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "# Loads in input file. It should be in format of:\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     URL         neighbor URL\n",
    "#     ...\n",
    "\n",
    "# The data file can be downloaded at http://www.cse.ust.hk/msbd5003/data/*\n",
    "lines = sc.textFile(\"../data/pagerank_data.txt\", 2)\n",
    "# lines = sc.textFile(\"../data/dblp.in\", 5)\n",
    "\n",
    "numOfIterations = 10\n",
    "\n",
    "# Loads all URLs from input file and initialize their neighbors. \n",
    "links = lines.map(lambda urls: parseNeighbors(urls)) \\\n",
    "             .groupByKey() # (1,(2,3)) (2,(4,1))\n",
    "\n",
    "# Loads all URLs with other URL(s) link to from input file \n",
    "# and initialize ranks of them to one.\n",
    "ranks = links.mapValues(lambda neighbors: 1.0) # (1,1.0) ....\n",
    "\n",
    "# Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "for iteration in range(numOfIterations):\n",
    "    # Calculates URL contributions to the rank of other URLs.\n",
    "    contribs = links.join(ranks) \\\n",
    "                    .flatMap(lambda url_urls_rank:\n",
    "                             computeContribs(url_urls_rank[1][0],\n",
    "                                             url_urls_rank[1][1]))\n",
    "    # After the join, each element in the RDD is of the form\n",
    "    # (url, (list of neighbor urls, rank))\n",
    "    \n",
    "    # Re-calculates URL ranks based on neighbor contributions.\n",
    "    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "    # ranks = contribs.reduceByKey(add).map(lambda (url, rank): (url, rank * 0.85 + 0.15))\n",
    "\n",
    "print ranks.top(5, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join vs. Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, ((134, 'OK'), 'Apple')), (1, ((135, 'OK'), 'Apple')), (1, ((45, 'OK'), 'Apple')), (2, ((53, 'OK'), 'Orange')), (3, ((34, 'OK'), 'TV')), (5, ((162, 'Error'), 'Computer'))]\n"
     ]
    }
   ],
   "source": [
    "products = sc.parallelize([(1, \"Apple\"), (2, \"Orange\"), (3, \"TV\"), (5, \"Computer\")])\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "print trans.join(products).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "products = {1: \"Apple\", 2: \"Orange\", 3: \"TV\", 5: \"Computer\"}\n",
    "trans = sc.parallelize([(1, (134, \"OK\")), (3, (34, \"OK\")), (5, (162, \"Error\")), (1, (135, \"OK\")), (2, (53, \"OK\")), (1, (45, \"OK\"))])\n",
    "\n",
    "# broadcasted_products = sc.broadcast(products)\n",
    "\n",
    "def f(x):\n",
    "    return (x[0], broadcasted_products.value[x[0]], x[1])\n",
    "\n",
    "results = trans.map(lambda x: (x[0], broadcasted_products.value[x[0]], x[1]))\n",
    "# results = trans.map(lambda x: (x[0], products[x[0]], x[1]))\n",
    "print results.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dsa'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'asd'\n",
    "a[::-1]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
