{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'several', u'unification')\n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "numPartitions = 10\n",
    "\n",
    "lines = sc.textFile('/Users/milkbread/adj_noun_pairs.txt', numPartitions)\n",
    "pairs = lines.map(lambda l: tuple(l.split())).filter(lambda p: len(p)==2)\n",
    "pairs.cache()\n",
    "\n",
    "# my code:\n",
    "# pairs.collect()\n",
    "# pairs.take(100) # 打印前100个元素\n",
    "# pairs = sc.parallelize(pairs, 4).cache()\n",
    "# 目前的思路是用forloop同时执行每个partition，然后把partition的结果合在一起比较\n",
    "def f(iterator):\n",
    "    for i in iterator:\n",
    "        if i[1] == \"unification\":\n",
    "            yield i\n",
    "            break\n",
    "\n",
    "SpecificNoun = pairs.mapPartitions(f).filter(lambda x: x[1] == \"unification\").first()\n",
    "\n",
    "print SpecificNoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.addPyFile('/Users/milkbread/graphframes-0.5.0-spark2.1-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q2\n",
    "sc.addPyFile(\"../graphframes-0.5.0-spark2.1-s_2.11.jar\")\n",
    "\n",
    "from graphframes import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Vertics DataFrame\n",
    "v = spark.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 37),\n",
    "  (\"d\", \"David\", 29),\n",
    "  (\"e\", \"Esther\", 32),\n",
    "  (\"f\", \"Fanny\", 38),\n",
    "  (\"g\", \"Gabby\", 60)\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# Edges DataFrame\n",
    "e = spark.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "  (\"f\", \"c\", \"follow\"),\n",
    "  (\"e\", \"f\", \"follow\"),\n",
    "  (\"e\", \"d\", \"friend\"),\n",
    "  (\"d\", \"a\", \"friend\"),\n",
    "  (\"a\", \"e\", \"friend\"),\n",
    "  (\"g\", \"e\", \"follow\")\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "\n",
    "# Create a GraphFrame\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# g.vertices.show()\n",
    "# g.edges.show()\n",
    "\n",
    "motifs = g.find(\"(a)-[]->(b); (b)-[]->(c)\")\\\n",
    "          .filter(\"a.name = 'Alice'\")\\\n",
    "          .select(\"c.name\") # edge不能相同\n",
    "motifs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q3\n",
    "motifs = g.find(\"(a)-[]->(b); (b)-[]->(c); (c)-[]->(a)\")\\\n",
    "          .filter(\"a.name = 'Alice'\")\\\n",
    "          .select(\"c.name\") # edge不能相同\n",
    "motifs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q4\n",
    "# people = g.edges.filter(\"relationship = 'follow' AND dst = 'Charlie'\")\n",
    "# people.show()\n",
    "peopele = e.join(v, e.src == v.id).filter(\"dst = 'c'and relationship = 'follow'\").select(\"name\")\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q5\n",
    "people = g.edges.filter(\"relationship = 'follow'\").groupBy('dst').withColumnRenamed('dst', 'leader').count()\n",
    "people.filter(\"count > 1\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
